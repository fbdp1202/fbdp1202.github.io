## Seq2Seq: RNN-Attention

![](1600920785671.png)

RNN에 단점
- 긴 history를 저장하기 힘들다.
- BTT 해줘야 해서 병렬 계산이 힘들다.

Transformers: Self Attention

![](1600920833208.png)

![](1600921079912.png)
